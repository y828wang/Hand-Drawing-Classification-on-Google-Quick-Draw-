{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\y884wang\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import keras.utils.np_utils as kutils\n",
    "from sklearn.model_selection import train_test_split\n",
    "import keras.callbacks as callbacks\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.callbacks import LearningRateScheduler, ModelCheckpoint\n",
    "from keras.utils import plot_model\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = np.load('apple.npy')\n",
    "potato = np.load('potato.npy')\n",
    "basketball = np.load('basketball.npy')\n",
    "watermelon = np.load('watermelon.npy')\n",
    "circle = np.load('circle.npy')\n",
    "compass = np.load('compass.npy')\n",
    "donut = np.load('donut.npy')\n",
    "soccerball = np.load('soccer ball.npy')\n",
    "wheel = np.load('wheel.npy')\n",
    "cookie = np.load('cookie.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "apple = np.c_[apple, np.zeros(apple.shape[0])]\n",
    "potato = np.c_[potato, np.ones(potato.shape[0])]\n",
    "basketball = np.c_[basketball, np.ones(basketball.shape[0])*2]\n",
    "watermelon = np.c_[watermelon, np.ones(watermelon.shape[0])*3]\n",
    "circle = np.c_[circle, np.ones(circle.shape[0])*4]\n",
    "compass = np.c_[compass, np.ones(compass.shape[0])*5]\n",
    "donut = np.c_[donut, np.ones(donut.shape[0])*6]\n",
    "soccerball = np.c_[soccerball, np.ones(soccerball.shape[0])*7]\n",
    "wheel = np.c_[wheel, np.ones(wheel.shape[0])*8]\n",
    "cookie = np.c_[cookie, np.ones(cookie.shape[0])*9]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data = np.concatenate((apple, potato,basketball,watermelon,circle,compass,donut,soccerball,wheel,cookie),axis=0)\n",
    "del apple, potato, basketball, watermelon, circle, compass, donut, soccerball, wheel, cookie\n",
    "X = Data[:,0:784]\n",
    "Y = Data[:,784]\n",
    "del Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.reshape(-1, 28,28, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1220204, 28, 28, 1)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reshape(-1,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(305051, 28, 28, 1)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train = X_train / 255.\n",
    "X_test = X_test / 255."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = kutils.to_categorical(Y_train)\n",
    "y_test = kutils.to_categorical(Y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_valid,y_train,y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(122021, 28, 28, 1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_valid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1098183, 10)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras.models import Sequential,Input,Model\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.advanced_activations import LeakyReLU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "epochs = 20\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D((2, 2),padding='same'))\n",
    "model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(128, activation='linear'))\n",
    "model.add(LeakyReLU(alpha=0.1))                  \n",
    "model.add(Dense(num_classes, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1098183 samples, validate on 122021 samples\n",
      "Epoch 1/20\n",
      "1098183/1098183 [==============================] - 1265s 1ms/step - loss: 0.4695 - acc: 0.8371 - val_loss: 0.3923 - val_acc: 0.8630\n",
      "\n",
      "Epoch 00001: val_acc improved from -inf to 0.86300, saving model to weights\n",
      "Epoch 2/20\n",
      "1098183/1098183 [==============================] - 1264s 1ms/step - loss: 0.3699 - acc: 0.8716 - val_loss: 0.3684 - val_acc: 0.8722\n",
      "\n",
      "Epoch 00002: val_acc improved from 0.86300 to 0.87218, saving model to weights\n",
      "Epoch 3/20\n",
      "1098183/1098183 [==============================] - 1257s 1ms/step - loss: 0.3448 - acc: 0.8800 - val_loss: 0.3581 - val_acc: 0.8746\n",
      "\n",
      "Epoch 00003: val_acc improved from 0.87218 to 0.87458, saving model to weights\n",
      "Epoch 4/20\n",
      "1098183/1098183 [==============================] - 1255s 1ms/step - loss: 0.3283 - acc: 0.8853 - val_loss: 0.3547 - val_acc: 0.8767\n",
      "\n",
      "Epoch 00004: val_acc improved from 0.87458 to 0.87670, saving model to weights\n",
      "Epoch 5/20\n",
      "1098183/1098183 [==============================] - 1218s 1ms/step - loss: 0.3161 - acc: 0.8892 - val_loss: 0.3522 - val_acc: 0.8794\n",
      "\n",
      "Epoch 00005: val_acc improved from 0.87670 to 0.87942, saving model to weights\n",
      "Epoch 6/20\n",
      "1098183/1098183 [==============================] - 1223s 1ms/step - loss: 0.3052 - acc: 0.8927 - val_loss: 0.3487 - val_acc: 0.8809\n",
      "\n",
      "Epoch 00006: val_acc improved from 0.87942 to 0.88091, saving model to weights\n",
      "Epoch 7/20\n",
      "1098183/1098183 [==============================] - 1199s 1ms/step - loss: 0.2958 - acc: 0.8958 - val_loss: 0.3530 - val_acc: 0.8789\n",
      "\n",
      "Epoch 00007: val_acc did not improve\n",
      "Epoch 8/20\n",
      "1098183/1098183 [==============================] - 1163s 1ms/step - loss: 0.2866 - acc: 0.8988 - val_loss: 0.3586 - val_acc: 0.8785\n",
      "\n",
      "Epoch 00008: val_acc did not improve\n",
      "Epoch 9/20\n",
      "1098183/1098183 [==============================] - 1277s 1ms/step - loss: 0.2790 - acc: 0.9015 - val_loss: 0.3632 - val_acc: 0.8797\n",
      "\n",
      "Epoch 00009: val_acc did not improve\n",
      "Epoch 10/20\n",
      "1098183/1098183 [==============================] - 1306s 1ms/step - loss: 0.2712 - acc: 0.9036 - val_loss: 0.3692 - val_acc: 0.8783\n",
      "\n",
      "Epoch 00010: val_acc did not improve\n",
      "Epoch 11/20\n",
      "1098183/1098183 [==============================] - 1292s 1ms/step - loss: 0.2641 - acc: 0.9059 - val_loss: 0.3841 - val_acc: 0.8744\n",
      "\n",
      "Epoch 00011: val_acc did not improve\n",
      "Epoch 12/20\n",
      "1098183/1098183 [==============================] - 1236s 1ms/step - loss: 0.2576 - acc: 0.9081 - val_loss: 0.3861 - val_acc: 0.8743\n",
      "\n",
      "Epoch 00012: val_acc did not improve\n",
      "Epoch 13/20\n",
      "1098183/1098183 [==============================] - 1216s 1ms/step - loss: 0.2509 - acc: 0.9101 - val_loss: 0.4013 - val_acc: 0.8735\n",
      "\n",
      "Epoch 00013: val_acc did not improve\n",
      "Epoch 14/20\n",
      "1098183/1098183 [==============================] - 1207s 1ms/step - loss: 0.2454 - acc: 0.9121 - val_loss: 0.4055 - val_acc: 0.8729\n",
      "\n",
      "Epoch 00014: val_acc did not improve\n",
      "Epoch 15/20\n",
      "1098183/1098183 [==============================] - 1241s 1ms/step - loss: 0.2394 - acc: 0.9140 - val_loss: 0.4067 - val_acc: 0.8727\n",
      "\n",
      "Epoch 00015: val_acc did not improve\n",
      "Epoch 16/20\n",
      "1098183/1098183 [==============================] - 1221s 1ms/step - loss: 0.2347 - acc: 0.9152 - val_loss: 0.4179 - val_acc: 0.8716\n",
      "\n",
      "Epoch 00016: val_acc did not improve\n",
      "Epoch 17/20\n",
      "1098183/1098183 [==============================] - 1243s 1ms/step - loss: 0.2294 - acc: 0.9169 - val_loss: 0.4358 - val_acc: 0.8718\n",
      "\n",
      "Epoch 00017: val_acc did not improve\n",
      "Epoch 18/20\n",
      "1098183/1098183 [==============================] - 1233s 1ms/step - loss: 0.2251 - acc: 0.9185 - val_loss: 0.4342 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00018: val_acc did not improve\n",
      "Epoch 19/20\n",
      "1098183/1098183 [==============================] - 1266s 1ms/step - loss: 0.2208 - acc: 0.9201 - val_loss: 0.4336 - val_acc: 0.8705\n",
      "\n",
      "Epoch 00019: val_acc did not improve\n",
      "Epoch 20/20\n",
      "1098183/1098183 [==============================] - 1239s 1ms/step - loss: 0.2162 - acc: 0.9216 - val_loss: 0.4616 - val_acc: 0.8688\n",
      "\n",
      "Epoch 00020: val_acc did not improve\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2c18123ef28>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "callbacks = [ModelCheckpoint('weights',\n",
    "                             monitor='val_acc',\n",
    "                             verbose=1,\n",
    "                             save_best_only=True,\n",
    "                            mode='auto')\n",
    "            ]\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=keras.optimizers.Adam(), metrics=[\"acc\"])\n",
    "model.fit(X_train, y_train, batch_size = batch_size, epochs=epochs,\n",
    "                   callbacks=callbacks,\n",
    "                   validation_data=(X_valid, y_valid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test loss: 0.4633625543305276\n",
      "Test accuracy: 0.8679958433166348\n"
     ]
    }
   ],
   "source": [
    "test_eval = model.evaluate(X_test, y_test, verbose=0)\n",
    "print('Test loss:', test_eval[0])\n",
    "print('Test accuracy:', test_eval[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
